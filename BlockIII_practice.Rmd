---
title: "Block III"
author: "Sara Dovalo and Javier MuÃ±oz Flores"
date: "21/3/2022"
output: 
  pdf_document:
    toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(h2o)
library(tidymodels)
library(dplyr)
library(ggplot2)
library(corrr)
```

\newpage

# Introduction

The content of this section is, mainly, a further description of the dataset used as well as the exposition of the problem to solve. 

The dataset selected contains several patient records of different medical measurements in order to predict wether a person is more likely to suffer a heart disease, i.e. a failure . The data has been retrieved from the public [*kaggle*](https://www.kaggle.com/datasets/fedesoriano/heart-failure-prediction) repository and it is the product of the combination of five different datasets from different regions of EEUU. The final dataset contains in total 918 instances and 12 attributes, which 7 of them are categorical and the five remaining are numerical:

 - `Age`(*quantitative*): age of the patient in years
 - `Sex`(*qualitative*): sex of the patient [*M*: Male, *F*: Female]
 - `ChestPainType`(*qualitative*): Angina type, i.e. chest pain, frequently caused when the heart muscle is not able to get enough oxygen-rich blood [*TA*: Typical Angina, *ATA*: Atypical Angina, NAP: Non-Anginal Pain, ASY: Asymptomatic]
 - `RestingBP`(*quantitative*): resting blood pressure [mm Hg]. A normal level is less than 180 mm Hg.
 - `Cholesterol`(*quantitative*): serum cholesterol [mm/dl]
 - `FastingBS`(*qualitative*): fasting blood sugar [1: if FastingBS > 120 mg/dl, 0: otherwise]
 - `RestingECG`(*qualitative*): resting electrocardiogram results [Normal: Normal, ST: having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of > 0.05 mV), LVH: showing probable or definite left ventricular hypertrophy by Estes' criteria]
 - `MaxHR`(*quantitative*): maximum heart rate achieved [Numeric value between 60 and 202]
 - `ExerciseAngina`: exercise-induced angina [Y: Yes, N: No]
 - `Oldpeak`(*quantitative*): oldpeak = ST [Numeric value measured in depression]
 - `ST_Slope`(*qualitative*): the slope of the peak exercise ST segment [Up: upsloping, Flat: flat, Down: downsloping]
 - `HeartDisease`(*qualitative*): class variable [1: heart disease, 0: Normal]

Clearly, it is a binary classification problem since the target variable has two levels.

# Preprocessing 

First of all, it is suitable to visualize the data and to identify if there are missing values which could add noise and disrupt the performance of the future models created.

```{r, include = FALSE, results = "hide"}
# Load data and rename first column
heart.data <- read.csv("heart.csv")
# Data summary
summary(heart.data)
```

We show the correlation between the numeric variables through a nice plot to carry out a first exploratory analysis.

```{r echo = FALSE, fig.align = "center", fig.height=5, fig.width=5, results = "hide", message=FALSE}
# Plot correlation
data_num <- heart.data %>%
select(where(is.numeric)) 
corr <- correlate(data_num)
rplot(corr, print_cor = TRUE)
```

We notice that the variable `Cholesterol` contains 172 values equal to zero. They must be considered as missing values as a person cannot that such a low level of cholesterol in blood. We decide to eliminate those observations that have a zero in that variable. 

```{r, results = "hide"}
# Count number of zeros (missing values)
heart.data %>% filter(Cholesterol == 0) %>% summarize(count = n())
# Eliminate rows which contain 0 in variable Cholesterol
heart.data = heart.data %>%
filter(Cholesterol != 0)
# Number of missing values
heart.data %>% filter(Cholesterol == 0) %>% summarize(count = n())
```


Furthermore, since the dataset includes several categorical attributes, it is necessary to transform them into *factors*. 

```{r warning=FALSE, message=FALSE, results = "hide"}
# Categorical variables as factors
heart.data = heart.data %>% 
       mutate_each_(funs(factor(.)),c(2,3,6,7,9,11,12))
str(heart.data)
```

To finish this section, we check if the classes of the response `HeartDisease` are balanced or not. We visualize it through a barplot to see it more clearly.

```{r, out.width = '50%', include = FALSE}
# Count the observation of each class
heart.data %>%
count(HeartDisease)
```
```{r echo=FALSE}
# Barplot
ggplot(heart.data, aes(HeartDisease)) + geom_bar()
```

The classes are balanced, it will be not needed to over/undersampling the sample.

# Modelling with `H2O` package

We follow the same procedure that we did in classroom for using `h2o.automl()`, i.e. fitting, benchmarking, predicting and explaining, in that order.

## Fitting 

Before starting with the selection of the models, the *local H2O cluster* is initialized in order to leverage the parallelization in the virtual machine which the package provides and to use H2O functions.

The first step is to convert the data into a `h2o`object and then, to identify the names of the predictors as well as the name of the response.

Then, it is important to mention that there will not be splitting into train and test, since we will use cross-validation metrics on the leaderboard model. Thus, we have to specify the number of folds (in our case will be `nfolds = 8`) needed to the cross-validation process. However, we do not have to indicate the predictors names, instead only the`dataFrame`and the response variable. In addition, we do not exclude any algorithm in `h2o.automl()` method, but we limit the time for fitting the models (`max_runtime_secs = 30` and `max_runtime_secs_per_model = 5`)

```{r, message=FALSE, include=FALSE}
# Initialize h2o
h2o.init()
h2o.no_progress()
# Send data to local H2O cluster
data.h <- as.h2o(heart.data)
# Data summary 
h2o.describe(data.h)
# Identify the names of the response and predictors
resp_h <- "HeartDisease"
pred_h<- setdiff(names(data.h), resp_h)
```

```{r, echo = FALSE, results = "hide"}
# Call h2o.automl()
model_h <- h2o.automl(y = resp_h, training_frame = data.h, max_runtime_secs = 30,max_runtime_secs_per_model = 5, nfolds = 8,
seed = 1, verbosity = NULL)
```

## Benchmarking

In this section, we have to explore the leaderboard of models in order to carry out a first comparison. We visualize the errors associated to each of the models

```{r, cache=TRUE, }
# Leaderboard
lead_h <- model_h@leaderboard
names(lead_h)[5] <- "mpce" # Rename mean_per_class_error to shorten output
print(lead_h[, -6], n = nrow(lead_h)) # Exclude final column to fit the table in one page
```

The leader has been a *StackedEnsemble* model with the lowest errors.

## Prediction

We select the leader of the models and predict a few rows.

```{r, cache=TRUE, results = "hide"}
h2o.predict(object = model_h@leader, newdata = data.h[1:8, ])
```

## Explanation

Because of the short report we must to hand in, it has not be possible to do a large interpretation of the plot results. However, it is suitable to identify what are the variables most important for the model and the correlation of them with the response. Thus, we use *SHAP* plot and *Variable Importance* plot to carry out a short explanation. 

```{r, warning=FALSE, out.width = '50%', message=FALSE, cache=TRUE}
exp <- h2o.explain(object = model_h, newdata = data.h)
exp
h2o.shutdown(prompt = FALSE)
```
 Clearly, the most important variable is *ST_Slope*, an important feature which can be seen in an electrocardiogram. As expected, these unbiased features related with the functionality of the heart have an crucial role in the prediction.
 
*SHAP* plot, for instance, reveals that women are more correlated with the response than men, a point that is not so clear in a first view.

\newpage

# Modelling with `tidymodels` package

`Tidymodels`is an interface that unifies hundreds of functions from different packages, facilitating all stages of pre-processing, training, optimization and validation of predictive models. 

The main packages that are part of the `tidymodels` ecosystem are:`brrom`, `rsample`, `parsnip`, `discrim`, `corr` y `tidypredict`, among others.

This package offers so many possibilities that they can hardly be shown with a single example.

The first step is to split the database into two subsets, the one used for training and the one used for validation. This is done with the `initial_split()` command of the `rsample` package. The training dataset, the one used for training the model and the one used to validate the model metrics can then be separated to help the selection, from a set of models applied on the same data, that one which performs best. It could be separated into two subsets (training and test), but it is a better practice to perform a cross-validation, so this one will be applied.

```{r}
# Partition on training and test
heart_split <- initial_split(heart.data, prop = .8)
data_train <- training(heart_split)
data_validate <- testing(heart_split)
```

For simplicity we only consider 7 of the 11 explanatory variables.

```{r}
variables <- c("Age", "Sex", "RestingBP", "Cholesterol", "FastingBS", 
               "RestingECG", "MaxHR", "HeartDisease")
data_train <- data_train %>% dplyr::select(all_of(variables))
data_validate <- data_validate %>% dplyr::select(all_of(variables))
```

## Logistic model

### Build a model

As the variable of interest `HeartDisease` is a binary variable, a logistic regression model is chosen, which will be our basis. 
An object shall be created that stores the formula for later use:

```{r, message=FALSE, warning=FALSE, cache=TRUE}
glm_fit <- glm(HeartDisease ~ Age + Sex + RestingBP + Cholesterol + FastingBS + 
                 RestingECG + MaxHR , data = data_train, family = binomial)
glm_fit_formula <- as.formula(HeartDisease ~ Age + Sex + RestingBP + Cholesterol +
                 FastingBS + RestingECG + MaxHR)
```


```{r}
summary(glm_fit)
tidy(glm_fit)
glance(glm_fit)
```



### Preprocess our data with recipes

The `resample` package will be used to evaluate the model. Ten cross-validations, each of 8 _folds_, will be used, therefore 80 samples will be obtained to evaluate the accuracy of the model.

```{r, message=FALSE, warning=FALSE, cache=TRUE}
set.seed(1234)
folds_with_repeats <- rsample::vfold_cv(data = data_train, v = 8, repeats = 10)
print(folds_with_repeats)
```

The command `rsample::vfold_cv()` generates the 80 subsets of equal size, varying the seed of the subdivision. The result generates an object with three variables: the repetition (`id`), the fold (`id2`) and splits. `splits` is a variable that stores a list of size (_v_*_repeats_)*3, where _v_ is the number of folds and _repeat_ the number of repetitions. Each element of the list is an object of the tibble class with several variables: 

- `data`: matrix of dimension n*p, where n is the number of records and p the number of variables.

- `in_id`: logical index of the position of the records in the data that belong to the records being analysed. This set of records is called analisys, and the one left out is called assessment. 

- `id`: stores the id of the fold and of the repetition to which the data corresponds.


### Prediction

Next, to assess the accuracy of the model for each of the samples generated with the cross-validation we will create a function that will mainly do the following: 

1. A logistic regression and model fit for the _analysis_ data.

2. Prediction on the _assessment_ data using one of the most prominent packages of the `tidymodels` package, `broom`.

3. Checking whether the prediction was performed correctly.

```{r}
res_leftout <- function(samplecv, model) {
  # Fit the model
  glm_model <- glm(model, data = analysis(samplecv), family = binomial)
  
  # Identify the dataset left out
  holdout <- assessment(samplecv)
  
  # Perfoms prediction on the data set hold out using augment()
  res <- broom::augment(glm_model, newdata = holdout)
  
  # lvls will be the levels of the factor with the predictions
  # the prediction (res$.fitted) is transformed into a nominal variable 
  # depending on whether it is greater or less than zero
  # If greater than zero, then No (no heart attack), otherwise Yes.
  lvls <- levels(holdout$HeartDisease)
  predictions <- factor(ifelse(res$.fitted > 0, lvls[2], lvls[1]), levels = lvls)
  
  # We check if the prediction is correct
  res$correct <- predictions == holdout$HeartDisease
  
  # Return the dataset with the additional columns
  res
}
```

For example, if we implement this procedure for the first of the 80 samples generated we obtain:

```{r}
firstsample <- res_leftout(folds_with_repeats$splits[[1]], glm_fit_formula)
dim(firstsample)
dim(rsample::assessment(folds_with_repeats$splits[[1]]))
```

Therefore, the name of the columns added for the first of the samples is:

```{r}
print(firstsample[1:7, setdiff("correct", names("HeartDisease"))])
```

As we can see, half of the predictions in the first sample are incorrect. For this particular model, `.fitted` refers to the linear predictor of the _log-odds_.

To do the above with the rest of the 79 samples obtained, we will apply the `purrr::map()` function, which applies the requested on each list:

```{r}
folds_with_repeats$results <- purrr::map(folds_with_repeats$splits, res_leftout,
                                glm_fit_formula)
print(folds_with_repeats)
```

We can now calculate the metric for all _assessment_ datasets. The percentage of heart diseases with a correct prediction is calculated:

```{r}
folds_with_repeats$results <- map_dbl(folds_with_repeats$results, function(x) {mean(x$correct)})
boxplot(folds_with_repeats$results, 
        main = 'Distribution of prediction according to the eighty subsets', 
        col = 'grey', ylab = 'Percentage', cex.main = 0.8, cex.lab = 0.8, cex.axis = 0.8)
```

The accuracy to be exceeded from this baseline is 0.73. Let's see if we can overcome it by applying a classification method such as k-means.

## Implementation of k-means algorithm

We attempt to implement a new method in order to achieve a further analysis of the data. Before that, we apply PCA and select the principal components which explain most of the variability of the dataset. To apply both techniques, it is compulsory to select the numeric attributes and highly recommendable to scale them.

```{r}
# PCA components
pca_comp <- heart.data %>%
select(where(is.numeric)) %>%
prcomp(scale. = TRUE)
tidy(pca_comp)
```

First, we plot the first two PCA components and visualize if the classes form differentiated clusters. By using the `augment()` function, we can complement the results as the PCA technique as the *k-means* algorithm. It is very useful in order to plot the categorization *k-means* in terms of the first two PCA components.

```{r}
aug_pca <- augment(pca_comp, data = heart.data)
ggplot(data = aug_pca, aes(x = .fittedPC1, y = .fittedPC2)) +
geom_point(aes(col = HeartDisease))
```

We cannot distinguish two groups in the data. We apply *k-means* method and again visualize through the first two PCA components the division in clusters which has carried out the algorithm.

```{r}
k_m <- heart.data %>%
select(where(is.numeric)) %>%
kmeans(centers = 2)
aug_k_m <- augment(k_m, data = heart.data)
aug_k_m <- cbind(aug_k_m, aug_pca[".fittedPC1"], aug_pca[".fittedPC2"])
ggplot(data = aug_k_m, mapping = aes(x = .fittedPC1, y = .fittedPC2)) +
geom_point(aes(col = .cluster, pch = HeartDisease))
```

It seems that now there is less overlapping, if we look at the cluster *k-means* division. It suggests that cluster *k-means* division may distinguish two different population in a better way than the classes do it.

# Appendix

```{r}
# Load data and rename first column
heart.data <- read.csv("heart.csv")
# Data summary
summary(heart.data)
# Plot correlation
data_num <- heart.data %>%
select(where(is.numeric)) 
corr <- correlate(data_num)
rplot(corr, print_cor = TRUE)
# Count number of zeros (missing values)
heart.data %>% filter(Cholesterol == 0) %>% summarize(count = n())
# Eliminate rows which contain 0 in variable Cholesterol
heart.data = heart.data %>%
filter(Cholesterol != 0)
# Number of missing values
heart.data %>% filter(Cholesterol == 0) %>% summarize(count = n())
# Categorical variables as factors
heart.data = heart.data %>% 
       mutate_each_(funs(factor(.)),c(2,3,6,7,9,11,12))
str(heart.data)
# Count the observation of each class
heart.data %>%
count(HeartDisease)
# Barplot
ggplot(heart.data, aes(HeartDisease)) + geom_bar()
